{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#-----------------------------------------------------------------------------------------------------#\n",
      "#-------------------------------Model Analysis and Testing With training.txt--------------------------#\n",
      "#-----------------------------------------------------------------------------------------------------#\n",
      "     Accuracy AVG Diviation\n",
      "LR : 0.981665 (0.008624)\n",
      "LDA: 0.844544 (0.038576)\n",
      "KNC: 0.974196 (0.014209)\n",
      "DTC: 0.890683 (0.034626)\n",
      "GNB: 0.479256 (0.044345)\n",
      "The best model results in a Logistic Regression Model.\n",
      "\n",
      "Sample Predictions: \n",
      "[-1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.\n",
      " -1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1. -1. -1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "\n",
      "Sample Actual:\n",
      "[-1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1. -1.  1.  1.\n",
      " -1. -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1. -1. -1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "\n",
      "Sample Accuracy:\n",
      "0.9701897018970189\n",
      "\n",
      "Sample Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.97      0.94      0.95       122\n",
      "         1.0       0.97      0.98      0.98       247\n",
      "\n",
      "    accuracy                           0.97       369\n",
      "   macro avg       0.97      0.96      0.97       369\n",
      "weighted avg       0.97      0.97      0.97       369\n",
      "\n",
      "\n",
      "\n",
      "#-----------------------------------------------------------------------------------------------------#\n",
      "#-----------------------------------Final Predictions For Test.txt------------------------------------#\n",
      "#-----------------------------------------------------------------------------------------------------#\n",
      "[-1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      " -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.\n",
      "  1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      "  1.  1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1.\n",
      " -1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
      "  1.  1. -1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.\n",
      "  1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Name: CS 458 Task 1 Testing File Predictions and Model Analysis\n",
    "#Authors: Liliana Pacheco, Chantelle Suarez, Yan Tarpley\n",
    "#Date: December 9, 2019\n",
    "#Description: This code takes in the training.txt file and testing.txt file. It then evaluates the training data\n",
    "#and processes the data and evaluates it to find the best possible model. It then makes preditions for the\n",
    "#testing.txt file \n",
    "#NOTE: The model analysis portion of this program was created using Jason Brownlee's article \n",
    "#\"How To Compare Machine Learning Algorithms in Python with scikit-learn\" as a referrence\n",
    "#Link to the article: https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------#\n",
    "#                                 May Change File Paths Here                                   #\n",
    "\n",
    "trainingFile = open(r\"C:\\Users\\lilia_fdv6j62\\Documents\\training.txt\",\"r+\")\n",
    "testingFile = open(r\"C:\\Users\\lilia_fdv6j62\\Documents\\testing.txt\", \"r+\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------#\n",
    "\n",
    "#Load Training  dataset \n",
    "trainingNames = ['info ID', 'ft ID', 'val']\n",
    "trainingDF = pd.read_csv(trainingFile, delimiter=\" \", names=trainingNames)\n",
    "trainingData = pd.pivot_table(trainingDF, index=['info ID'], columns=['ft ID'], values=['val'])\n",
    "trainingData.fillna(0, inplace=True)\n",
    "\n",
    "#Load Testing Dataset\n",
    "testingNames = ['info ID', 'ft ID', 'val']\n",
    "testingDF = pd.read_csv(testingFile, delimiter=\" \", names=testingNames)\n",
    "testingData = pd.pivot_table(testingDF, index=['info ID'], columns=['ft ID'], values=['val'])\n",
    "testingData.fillna(0, inplace=True)\n",
    "\n",
    "#Load Training Labels\n",
    "trainingLabelFile = open(r\"C:\\Users\\lilia_fdv6j62\\Documents\\label_training.txt\", 'r+')\n",
    "labelNames = ['label']\n",
    "trainingLabelDF = pd.read_csv(trainingLabelFile, delimiter=\" \", names=labelNames)\n",
    "\n",
    "#Merge Trainning Data and Training Labels\n",
    "trainingLabelDF.insert(0, 'info ID', range(1, len(trainingLabelDF) + 1))\n",
    "trainingMerge = pd.merge(trainingLabelDF, trainingData, on=\"info ID\")\n",
    "cols = list(trainingMerge.columns.to_numpy())\n",
    "trainingLabelCol = trainingMerge.pop('label')\n",
    "trainingMerge['label'] = trainingLabelCol\n",
    "\n",
    "#Process Training Data with PCA\n",
    "trainingFeatures = list(trainingMerge.columns)\n",
    "trainingTarget = trainingFeatures.pop()\n",
    "\n",
    "x = trainingMerge.loc[:, trainingFeatures].values\n",
    "y = trainingMerge.loc[:,['label']].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "numComponents = 55\n",
    "pca = PCA(n_components=numComponents)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDF = pd.DataFrame(data=principalComponents, columns = range(1,numComponents + 1))\n",
    "\n",
    "finalDF = pd.concat([principalDF, trainingMerge[['label']]], axis=1)\n",
    "\n",
    "#Process Testing Data with PCA\n",
    "testingFeatures = list(testingData.columns)\n",
    "x2 = testingData.loc[:, testingFeatures].values\n",
    "x2 = StandardScaler().fit_transform(x2)\n",
    "\n",
    "testingPCA = PCA(n_components = numComponents)\n",
    "testingPrincipalComponents  = testingPCA.fit_transform(x2)\n",
    "testingFinalDF = pd.DataFrame(data = testingPrincipalComponents, columns = range(1, numComponents + 1))\n",
    "\n",
    "#Split Training Data For Model Analysis\n",
    "finalData = finalDF.values\n",
    "xVal = finalData[:,0:numComponents]\n",
    "yVal= finalData[:,numComponents]\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xVal, yVal, test_size=0.20, random_state=1)\n",
    "\n",
    "#Select Models For Testing\n",
    "models = []\n",
    "models.append(('LR ', LogisticRegression(solver='liblinear',multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNC', KNeighborsClassifier()))\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "\n",
    "#Evaluate Each Model\n",
    "print(\"\\n\\n#-----------------------------------------------------------------------------------------------------#\")\n",
    "print(\"#-------------------------------Model Analysis and Testing With training.txt--------------------------#\")\n",
    "print(\"#-----------------------------------------------------------------------------------------------------#\")\n",
    "print(\"     Accuracy AVG Diviation\")\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=0)\n",
    "    cv_results = model_selection.cross_val_score(model, xTrain, yTrain, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "#Select The Best Model For Testing\n",
    "print(\"The best model results in a Logistic Regression Model.\")\n",
    "testModel = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "testModel.fit(xTrain, yTrain)\n",
    "\n",
    "#Make Sample Predictions\n",
    "testPredictions = testModel.predict(xTest)\n",
    "print(\"\\nSample Predictions: \")\n",
    "print(testPredictions)\n",
    "print(\"\\nSample Actual:\")\n",
    "print(yTest)\n",
    "\n",
    "# Evaluate Sample predictions\n",
    "print(\"\\nSample Accuracy:\")\n",
    "print(accuracy_score(yTest, testPredictions))\n",
    "\n",
    "print(\"\\nSample Report\")\n",
    "print(classification_report(yTest, testPredictions))\n",
    "\n",
    "#Make Final Predictions for Test.txt\n",
    "finalModel = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "finalModel.fit(xVal, yVal)\n",
    "\n",
    "finalPredictions = finalModel.predict(testingFinalDF)\n",
    "print(\"\\n\\n#-----------------------------------------------------------------------------------------------------#\")\n",
    "print(\"#-----------------------------------Final Predictions For Test.txt------------------------------------#\")\n",
    "print(\"#-----------------------------------------------------------------------------------------------------#\")\n",
    "print(finalPredictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
